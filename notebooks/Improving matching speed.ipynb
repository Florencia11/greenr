{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving ingredient category matching speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T13:16:36.654613Z",
     "start_time": "2020-08-31T13:16:35.873830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yanni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yanni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yanni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yanni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import some packages\n",
    "\n",
    "import _pickle as cPickle\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import os.path\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "# Load some data, define some values\n",
    "\n",
    "vectorizer = cPickle.load(open('../greenr/vectorizer.pk', 'rb'))\n",
    "df_wiki_similarities = cPickle.load(open('../greenr/df_wiki_similarities.pk', 'rb'))\n",
    "df_recorded_similarities = cPickle.load(open('../greenr/df_recorded_similarities.pk', 'rb'))\n",
    "\n",
    "api_key = cPickle.load(open('../greenr/api_key.pk', 'rb'))\n",
    "cse_id = \"dd94ab4664d1ce589\"\n",
    "\n",
    "catsums = df_wiki_similarities['summaries'][:45]\n",
    "cats = list(df_wiki_similarities[df_wiki_similarities['ingr/cat'] == 'cat']\n",
    "            ['ingredient'])\n",
    "catvectors = vectorizer.transform(catsums)\n",
    "\n",
    "similarity_cutoff = 0.1\n",
    "no_match = 'No match found'\n",
    "\n",
    "# Define some utility functions\n",
    "\n",
    "def is_ingredient_in_database(ingredient):\n",
    "    found = ingredient in list(df_recorded_similarities.ingredient)\n",
    "    return found\n",
    "\n",
    "def get_database_match(ingredient):\n",
    "    \n",
    "    match = df_recorded_similarities.loc[df_recorded_similarities['ingredient'] == ingredient, 'category'].iloc[0]\n",
    "    \n",
    "    return match\n",
    "\n",
    "def is_ingredient_in_wikidata(ingredient):\n",
    "    found = ingredient in list(df_wiki_similarities.ingredient)\n",
    "    return found\n",
    "\n",
    "def get_wiki_match(ingredient):\n",
    "\n",
    "    i_ix = list(df_wiki_similarities.ingredient).index(ingredient)\n",
    "\n",
    "    chosen_summ = df_wiki_similarities.summaries[i_ix]\n",
    "\n",
    "    sims = df_wiki_similarities.iloc[i_ix, 4:]\n",
    "\n",
    "    c_ix = pd.to_numeric(sims).argmax()\n",
    "\n",
    "    ingredient = df_wiki_similarities['ingredient'][i_ix]\n",
    "    category = df_wiki_similarities['ingredient'][c_ix]\n",
    "\n",
    "    return category, max(sims)\n",
    "\n",
    "def google_query(query, api_key, cse_id, **kwargs):\n",
    "\n",
    "    query_service = build(\"customsearch\", \"v1\", developerKey=api_key, cache_discovery = False)\n",
    "    query_results = query_service.cse().list(q=query, cx=cse_id,\n",
    "                                             **kwargs).execute()\n",
    "\n",
    "    return query_results['items']\n",
    "\n",
    "\n",
    "def get_google_cse_result(ingredient):\n",
    "\n",
    "    query = f'{ingredient} food'\n",
    "\n",
    "    my_results = google_query(query, api_key, cse_id, num=1)[0]\n",
    "\n",
    "    url = my_results['link']\n",
    "    url_base = os.path.basename(my_results['link'])\n",
    "\n",
    "    return ingredient, url, url_base\n",
    "\n",
    "def get_pageid_from_base(base):\n",
    "\n",
    "    info_url = f'https://en.wikipedia.org/w/index.php?title={base}&action=info'\n",
    "\n",
    "    req = urllib.request.Request(info_url)\n",
    "    req.add_header('Cookie', 'euConsent=true')\n",
    "    html_content = urllib.request.urlopen(req).read()\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    infosection = soup.find(\"script\")\n",
    "    pageid = infosection.decode().partition('wgArticleId\":')[2].partition(\n",
    "        ',')[0]\n",
    "\n",
    "    return pageid\n",
    "\n",
    "def get_summary_from_id(pageid):\n",
    "\n",
    "    pagesummary = wikipedia.page(pageid=pageid).summary\n",
    "\n",
    "    return pagesummary\n",
    "\n",
    "def pre_process_summary(summary):\n",
    "\n",
    "    # Remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        summary = str(summary).replace(punctuation, '')\n",
    "\n",
    "    # Lower text\n",
    "    summary = summary.lower()\n",
    "\n",
    "    # Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    summary_tokenized = word_tokenize(summary)\n",
    "    text = [w for w in summary_tokenized if not w in stop_words]\n",
    "    summary = ' '.join(text)\n",
    "\n",
    "    # Remove digits\n",
    "    summary = ''.join([word for word in summary if not word.isdigit()])\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    summary = ' '.join([lemmatizer.lemmatize(word) for word in summary.split(' ')])\n",
    "\n",
    "    # Keep only nouns\n",
    "    tokens = summary.split()\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    summary = [\n",
    "        word for word, pos in tags\n",
    "        if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')\n",
    "    ]\n",
    "\n",
    "    summary = ' '.join(summary)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def get_match_and_score(summary_vector):\n",
    "\n",
    "    scoreseries = []\n",
    "\n",
    "    for j, catsum in enumerate(catsums):\n",
    "\n",
    "        cosine_sum = 1 - spatial.distance.cosine(summary_vector.toarray(),\n",
    "                                                 catvectors[j, :].toarray())\n",
    "\n",
    "        scoreseries.append(cosine_sum)\n",
    "\n",
    "    matchscore = max(scoreseries)\n",
    "    match = cats[scoreseries.index(matchscore)]\n",
    "\n",
    "    return match, matchscore\n",
    "\n",
    "def get_google_match(ingredient):\n",
    "\n",
    "    try:\n",
    "        ingredient, url, url_base = get_google_cse_result(ingredient)\n",
    "    except:\n",
    "        return 'nomatch', 0\n",
    "\n",
    "    pageid = get_pageid_from_base(url_base)\n",
    "\n",
    "    pagesummary = get_summary_from_id(pageid)\n",
    "\n",
    "    processed_summary = pre_process_summary(pagesummary)\n",
    "\n",
    "    summary_vector = vectorizer.transform([processed_summary])\n",
    "\n",
    "    match, matchscore = get_match_and_score(summary_vector)\n",
    "\n",
    "    return match, matchscore\n",
    "\n",
    "def update_database(ingredient, match):\n",
    "    \n",
    "    global df_recorded_similarities\n",
    "    \n",
    "    df_tmp = pd.DataFrame([[ingredient, match]], columns = ['ingredient','category'])\n",
    "                        \n",
    "    df_recorded_similarities = df_recorded_similarities.append(df_tmp, ignore_index=True)\n",
    "            \n",
    "    cPickle.dump(df_recorded_similarities, open(\"../greenr/df_recorded_similarities.pk\", \"wb\"))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Define the matching function\n",
    "\n",
    "def get_categories(df_parser_output, try_google=False):\n",
    "    \n",
    "    matched_categories = []\n",
    "\n",
    "    list_of_ingredients = list(df_parser_output['name'])\n",
    "\n",
    "    for ingredient in list_of_ingredients:\n",
    "                \n",
    "        if is_ingredient_in_database(ingredient):\n",
    "            \n",
    "            match = get_database_match(ingredient)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if is_ingredient_in_wikidata(ingredient):\n",
    "\n",
    "                wikimatch, score = get_wiki_match(ingredient)\n",
    "\n",
    "                if score > similarity_cutoff:\n",
    "                    match = wikimatch\n",
    "\n",
    "                elif try_google:\n",
    "                    googlematch, score = get_google_match(ingredient)\n",
    "                    if score > similarity_cutoff:\n",
    "                        match = googlematch\n",
    "                    else:\n",
    "                        match = no_match\n",
    "\n",
    "                else:\n",
    "                    match = no_match\n",
    "\n",
    "            elif try_google:\n",
    "\n",
    "                googlematch, score = get_google_match(ingredient)\n",
    "\n",
    "                if score > similarity_cutoff:\n",
    "                    match = googlematch\n",
    "\n",
    "                else:\n",
    "                    match = no_match\n",
    "\n",
    "            else:\n",
    "                match = no_match\n",
    "\n",
    "            update_database(ingredient, match)            \n",
    "            \n",
    "        matched_categories.append(match)\n",
    "                    \n",
    "    for i,cat in enumerate(matched_categories):\n",
    "        if cat == 'Onions & leeks':\n",
    "            matched_categories[i] = 'Onions & Leeks'\n",
    "\n",
    "        if cat == 'Berries & Grapes2':\n",
    "            matched_categories[i] = 'Berries & Grapes'\n",
    "    \n",
    "    return matched_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T13:16:51.625060Z",
     "start_time": "2020-08-31T13:16:51.621056Z"
    }
   },
   "outputs": [],
   "source": [
    "add = ['dummy_qty','dummy_unit']\n",
    "\n",
    "col1 = ['wagyu ribeye'] + add\n",
    "col2 = ['allpurpose flour'] + add\n",
    "col3 = ['kohlrabi'] + add\n",
    "\n",
    "data = {'col_1': col1, 'col_2': col2, 'col_3': col3}\n",
    "\n",
    "testdf = pd.DataFrame.from_dict(data, orient = 'index', columns=['name', 'quantity', 'unit'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T13:16:54.073957Z",
     "start_time": "2020-08-31T13:16:51.977503Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bovine Meat (beef herd)', 'Maize (Meal)', 'Brassicas']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_categories(testdf, try_google=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T13:16:55.205322Z",
     "start_time": "2020-08-31T13:16:55.195515Z"
    }
   },
   "outputs": [],
   "source": [
    "df_recorded_similarities = cPickle.load(open('../greenr/df_recorded_similarities.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T13:16:55.432762Z",
     "start_time": "2020-08-31T13:16:55.423766Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ingredient</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ground chuck</td>\n",
       "      <td>Bovine Meat (beef herd)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all-purpose flour</td>\n",
       "      <td>Wheat &amp; Rye (Bread)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kohlrabi</td>\n",
       "      <td>Brassicas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allpurpose flour</td>\n",
       "      <td>Maize (Meal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ground pork</td>\n",
       "      <td>Lamb &amp; Mutton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lamb rack</td>\n",
       "      <td>Lamb &amp; Mutton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iberico secreto</td>\n",
       "      <td>Pig Meat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wagyu ribeye</td>\n",
       "      <td>Bovine Meat (beef herd)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ingredient                 category\n",
       "0       ground chuck  Bovine Meat (beef herd)\n",
       "1  all-purpose flour      Wheat & Rye (Bread)\n",
       "2           kohlrabi                Brassicas\n",
       "3   allpurpose flour             Maize (Meal)\n",
       "4        ground pork            Lamb & Mutton\n",
       "5          lamb rack            Lamb & Mutton\n",
       "6    iberico secreto                 Pig Meat\n",
       "7       wagyu ribeye  Bovine Meat (beef herd)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recorded_similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Greenr",
   "language": "python",
   "name": "greenr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
